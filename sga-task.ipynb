{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a website. It allows you to understand how users interact with the site. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_type (string)` - Page on the site.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leverage. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.e.: no O^2 or O^3 complexities; appropriate object usage; no data leaks etc. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code etc. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.10/site-packages (3.1.3)\n",
      "Requirement already satisfied: py4j==0.10.9 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root supergroup   32241574 2023-09-24 20:38 /data/clickstream.csv\n",
      "drwxr-xr-x   - root supergroup          0 2023-09-24 20:38 /data/transactions\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup   32241574 2023-09-24 20:38 /data/clickstream.csv\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /data/clickstream.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyToLocal: `clickstream.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -copyToLocal /data/clickstream.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-01-21 11:23:59,193 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='ClickstreamAnalysis')\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "se = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL Solution (with <=2 queries):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "clickstream = se.read.csv(\"hdfs:///data/clickstream.csv\", header=True, inferSchema=True, sep=\"\\t\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clickstream.registerTempTable(\"clicks_reg_tb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>event_type</th>\n",
       "      <th>event_page</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>562</td>\n",
       "      <td>507</td>\n",
       "      <td>page</td>\n",
       "      <td>main</td>\n",
       "      <td>1695584127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>562</td>\n",
       "      <td>507</td>\n",
       "      <td>event</td>\n",
       "      <td>main</td>\n",
       "      <td>1695584134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  session_id event_type event_page   timestamp\n",
       "0      562         507       page       main  1695584127\n",
       "1      562         507      event       main  1695584134"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clickstream.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    route  count\n",
      "0                    main   8184\n",
      "1            main-archive   1113\n",
      "2             main-rabota   1047\n",
      "3           main-internet    897\n",
      "4              main-bonus    870\n",
      "5               main-news    769\n",
      "6            main-tariffs    677\n",
      "7             main-online    587\n",
      "8              main-vklad    518\n",
      "9     main-rabota-archive    170\n",
      "10    main-archive-rabota    167\n",
      "11     main-bonus-archive    143\n",
      "12      main-rabota-bonus    139\n",
      "13      main-bonus-rabota    135\n",
      "14       main-news-rabota    135\n",
      "15  main-archive-internet    132\n",
      "16       main-rabota-news    130\n",
      "17   main-internet-rabota    129\n",
      "18      main-archive-news    126\n",
      "19   main-rabota-internet    124\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result_sql' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Write the result to a file\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult_sql.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m---> 46\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mresult_sql\u001b[49m\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m30\u001b[39m)\u001b[38;5;241m.\u001b[39mtoPandas()\u001b[38;5;241m.\u001b[39mto_csv(sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result_sql' is not defined"
     ]
    }
   ],
   "source": [
    "# Write SQL queries to process the data and find the most frequent routes\n",
    "query = \"\"\"\n",
    "    WITH add_error_flg as (\n",
    "            SELECT\n",
    "                user_id,\n",
    "                session_id,\n",
    "                event_page,\n",
    "                timestamp,\n",
    "                MAX(CASE WHEN event_type LIKE \"%error%\" THEN 1 ELSE 0 END) OVER (PARTITION BY user_id, session_id ORDER BY timestamp) as error_flg,\n",
    "                LAG(event_page) OVER (PARTITION BY user_id, session_id ORDER BY timestamp) as last_page\n",
    "            FROM clicks_reg_tb\n",
    "        ),\n",
    "        collect_route AS (\n",
    "            SELECT\n",
    "                user_id,\n",
    "                session_id,\n",
    "                ROW_NUMBER() OVER (PARTITION BY user_id, session_id ORDER BY timestamp DESC) as rn,\n",
    "                COLLECT_LIST(event_page) OVER (PARTITION BY user_id, session_id ORDER BY timestamp) as route_list\n",
    "            FROM add_error_flg\n",
    "            WHERE 1=1\n",
    "                AND error_flg = 0\n",
    "                AND (last_page != event_page OR last_page IS NULL)\n",
    "        ),\n",
    "        agg_route AS (\n",
    "            SELECT\n",
    "                user_id,\n",
    "                session_id,\n",
    "                CONCAT_WS(\\\"-\\\", route_list) as route\n",
    "            FROM collect_route\n",
    "            WHERE rn = 1\n",
    "        )\n",
    "        SELECT\n",
    "            route,\n",
    "            COUNT(session_id) as count\n",
    "        FROM agg_route\n",
    "        GROUP BY route\n",
    "        ORDER BY count DESC\n",
    "        ;\n",
    "\"\"\"\n",
    "\n",
    "result_sql = se.sql(query)\n",
    "print(result_sql.limit(20).toPandas())\n",
    "\n",
    "# Write the result to a file\n",
    "with open('result_sql.csv', 'w') as file:\n",
    "    file.write(result_sql.limit(30).toPandas().to_csv(sep='\\t', index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark RDD solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+----------+----------+\n",
      "|user_id|session_id|  event_type|event_page| timestamp|\n",
      "+-------+----------+------------+----------+----------+\n",
      "|    562|       507|        page|      main|1695584127|\n",
      "|    562|       507|       event|      main|1695584134|\n",
      "|    562|       507|       event|      main|1695584144|\n",
      "|    562|       507|       event|      main|1695584147|\n",
      "|    562|       507|wNaxLlerrorU|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584160|\n",
      "|    562|       507|        page|    rabota|1695584166|\n",
      "|    562|       507|       event|    rabota|1695584174|\n",
      "+-------+----------+------------+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               route|count|\n",
      "+--------------------+-----+\n",
      "|                main| 8184|\n",
      "|        main-archive| 1113|\n",
      "|         main-rabota| 1047|\n",
      "|       main-internet|  897|\n",
      "|          main-bonus|  870|\n",
      "|           main-news|  769|\n",
      "|        main-tariffs|  677|\n",
      "|         main-online|  587|\n",
      "|          main-vklad|  518|\n",
      "| main-rabota-archive|  170|\n",
      "| main-archive-rabota|  167|\n",
      "|  main-bonus-archive|  143|\n",
      "|   main-rabota-bonus|  139|\n",
      "|    main-news-rabota|  135|\n",
      "|   main-bonus-rabota|  135|\n",
      "|main-archive-inte...|  132|\n",
      "|    main-rabota-news|  130|\n",
      "|main-internet-rabota|  129|\n",
      "|   main-archive-news|  126|\n",
      "|main-rabota-internet|  124|\n",
      "|main-internet-arc...|  123|\n",
      "|  main-archive-bonus|  117|\n",
      "| main-internet-bonus|  115|\n",
      "|main-tariffs-inte...|  114|\n",
      "|   main-news-archive|  113|\n",
      "|  main-news-internet|  109|\n",
      "|main-archive-tariffs|  104|\n",
      "|  main-internet-news|  103|\n",
      "|main-tariffs-archive|  103|\n",
      "|    main-rabota-main|   94|\n",
      "+--------------------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------                                        \n",
      "Exception occurred during processing of request from ('127.0.0.1', 53762)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "clicks = (\n",
    "    se.read\n",
    "    .csv('hdfs:///data/clickstream.csv', sep='\\t', header=True, inferSchema=True))\n",
    "clicks.limit(10).show()\n",
    "\n",
    "window = Window.partitionBy('user_id', 'session_id')\n",
    "\n",
    "ad_er = (\n",
    "    clicks\n",
    "    .withColumn(\n",
    "        'error_flg',\n",
    "        F.max(\n",
    "            F.when(\n",
    "                F.col('event_type').like('%error%'), 1).otherwise(0)\n",
    "        ).over(window.orderBy('timestamp'))\n",
    "    )\n",
    "    .withColumn(\n",
    "        'last_page',\n",
    "        F.lag('event_page').over(window.orderBy('timestamp'))\n",
    "    )\n",
    ")\n",
    "\n",
    "col_ro = (\n",
    "    ad_er\n",
    "    .filter('error_flg = 0 and (last_page != event_page or last_page is null)')\n",
    "    .withColumn(\n",
    "        'rn',\n",
    "        F.row_number().over(window.orderBy(F.desc('timestamp')))\n",
    "    )\n",
    "    .withColumn(\n",
    "        'route_list',\n",
    "        F.collect_list('event_page').over(window.orderBy('timestamp'))\n",
    "    )\n",
    "    .filter('rn = 1')\n",
    ")\n",
    "\n",
    "ag_ro = (\n",
    "    col_ro\n",
    "    .select(\n",
    "        'user_id',\n",
    "        'session_id',\n",
    "        F.concat_ws('-', 'route_list').alias('route')\n",
    "    )\n",
    ")\n",
    "\n",
    "result = (\n",
    "    ag_ro\n",
    "    .groupBy('route')\n",
    "    .agg(F.count('session_id').alias('count'))\n",
    "    .orderBy(F.desc('count'))\n",
    ")\n",
    "\n",
    "result.show(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_df.csv', 'w') as file:\n",
    "    file.write(result_df.limit(30).toPandas().to_csv(sep='\\t', index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark RDD Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['user_id', 'session_id', 'event_type', 'event_page', 'timestamp'],\n",
       " ['562', '507', 'page', 'main', '1695584127'],\n",
       " ['562', '507', 'event', 'main', '1695584134'],\n",
       " ['562', '507', 'event', 'main', '1695584144'],\n",
       " ['562', '507', 'event', 'main', '1695584147'],\n",
       " ['562', '507', 'wNaxLlerrorU', 'main', '1695584154'],\n",
       " ['562', '507', 'event', 'main', '1695584154'],\n",
       " ['562', '507', 'event', 'main', '1695584154'],\n",
       " ['562', '507', 'event', 'main', '1695584160'],\n",
       " ['562', '507', 'page', 'rabota', '1695584166']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks_rdd = sc.textFile(\"hdfs:/data/clickstream.csv\").map(lambda line: line.split(\"\\t\"))\n",
    "clicks_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((562, 507), (1695584127, 'page', 'main')),\n",
       " ((562, 507), (1695584134, 'event', 'main')),\n",
       " ((562, 507), (1695584144, 'event', 'main')),\n",
       " ((562, 507), (1695584147, 'event', 'main')),\n",
       " ((562, 507), (1695584154, 'wNaxLlerrorU', 'main')),\n",
       " ((562, 507), (1695584154, 'event', 'main')),\n",
       " ((562, 507), (1695584154, 'event', 'main')),\n",
       " ((562, 507), (1695584160, 'event', 'main')),\n",
       " ((562, 507), (1695584166, 'page', 'rabota'))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_rdd = (\n",
    "    clicks_rdd\n",
    "    .filter(lambda x: x[0] != 'user_id')\n",
    "    .map(lambda x: ((int(x[0]), int(x[1])), (int(x[4]), x[2], x[3])))\n",
    "    .partitionBy(2, partitionFunc=lambda x: x[0] % 2)\n",
    ")\n",
    "filtered_rdd.take(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
